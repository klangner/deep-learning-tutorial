{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 'a'), ('a', 'n'), ('n', 'a'), ('a', 'r'), ('r', 'c'), ('c', 'h'), ('h', 'i'), ('i', 's'), ('s', 'm'), ('m', ' '), (' ', 'o'), ('o', 'r'), ('r', 'i'), ('i', 'g'), ('g', 'i'), ('i', 'n'), ('n', 'a'), ('a', 't'), ('t', 'e'), ('e', 'd')]\n",
      "Data2 size 99999999\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "text2 = zip(text, text[1:])\n",
    "print(text2[:20])\n",
    "print('Data2 size %d' % len(text2))\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998999 [('o', 'n'), ('n', 's'), ('s', ' '), (' ', 'a'), ('a', 'n'), ('n', 'a'), ('a', 'r'), ('r', 'c'), ('c', 'h'), ('h', 'i'), ('i', 's'), ('s', 't'), ('t', 's'), ('s', ' '), (' ', 'a'), ('a', 'd'), ('d', 'v'), ('v', 'o'), ('o', 'c'), ('c', 'a'), ('a', 't'), ('t', 'e'), ('e', ' '), (' ', 's'), ('s', 'o'), ('o', 'c'), ('c', 'i'), ('i', 'a'), ('a', 'l'), ('l', ' '), (' ', 'r'), ('r', 'e')]\n",
      "1000 [(' ', 'a'), ('a', 'n'), ('n', 'a'), ('a', 'r'), ('r', 'c'), ('c', 'h'), ('h', 'i'), ('i', 's'), ('s', 'm'), ('m', ' '), (' ', 'o'), ('o', 'r'), ('r', 'i'), ('i', 'g'), ('g', 'i'), ('i', 'n'), ('n', 'a'), ('a', 't'), ('t', 'e'), ('e', 'd'), ('d', ' '), (' ', 'a'), ('a', 's'), ('s', ' '), (' ', 'a'), ('a', ' '), (' ', 't'), ('t', 'e'), ('e', 'r'), ('r', 'm'), ('m', ' '), (' ', 'o')]\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text2[:valid_size]\n",
    "train_text = text2[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:32])\n",
    "print(valid_size, valid_text[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 80\n",
      "(' ', 'a') ('b', 'z')\n"
     ]
    }
   ],
   "source": [
    "letter_count = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "vocabulary_size = letter_count**2\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char[0]) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def chars2id(chars):\n",
    "    return char2id(chars[0])*letter_count + char2id(chars[1])\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "def id2chars(dictid):\n",
    "    c1 = id2char(dictid // letter_count)\n",
    "    c2 = id2char(dictid % letter_count)\n",
    "    return (c1, c2)\n",
    "\n",
    "print(chars2id((' ', 'a')), chars2id(('b', 'z')))\n",
    "print(id2chars(1), id2chars(80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gram embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model.\n",
    "  * *num_unrollings* defines number of LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 419.  629.  336.    1.  352.  221.  675.   52.  549.  360.  383.  221.\n",
      "  135.  137.  405.  680.  423.   27.   20.   45.  263.    1.  549.  133.\n",
      "  162.   47.  135.  149.   40.  508.  258.  405.   27.  197.  257.  501.\n",
      "   82.   46.    4.  366.  540.  567.  135.  405.  411.  513.  309.  153.\n",
      "  640.  155.  548.  155.   19.  262.  549.  108.  548.  149.  167.  129.\n",
      "  558.   47.   43.  522.]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        # Segment is number of batches in the text\n",
    "        segment = self._text_size // batch_size\n",
    "        # Cursor is a list of pointers set to the beggining of each segment\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = chars2id(self._text[self._cursor[b]])\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "    \n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "b = train_batches.next()\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_distribution([0.1, 0.2, 0.3, 0.4]) = 2\n",
      "random_dist =  [[ 0.01386932  0.01533595  0.08415141  0.03484591  0.0203517   0.06124168\n",
      "   0.02061342  0.03298343  0.0007279   0.0109498   0.00705117  0.0801856\n",
      "   0.07798728  0.02601601  0.03483116  0.06856687  0.05256913  0.0028407\n",
      "   0.08185048  0.06418326  0.03366877  0.08387608  0.00085087  0.04349868\n",
      "   0.01351004  0.00875995  0.02468342]]\n",
      "sample(random_dist) =  [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# def logprob(predictions, labels):\n",
    "#     \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "#     predictions[predictions < 1e-10] = 1e-10\n",
    "#     return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "# def sample_distribution(distribution):\n",
    "#     \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "#     probabilities.\n",
    "#     \"\"\"\n",
    "#     r = random.uniform(0, 1)\n",
    "#     s = 0\n",
    "#     for i in range(len(distribution)):\n",
    "#         s += distribution[i]\n",
    "#         if s >= r:\n",
    "#             return i\n",
    "#     return len(distribution) - 1\n",
    "\n",
    "# def sample(prediction):\n",
    "#     \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "#     p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "#     p[0, sample_distribution(prediction[0])] = 1.0\n",
    "#     return p\n",
    "\n",
    "# def random_distribution():\n",
    "#     \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "#     b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "#     return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "# print('sample_distribution([0.1, 0.2, 0.3, 0.4]) = %d' % sample_distribution([0.1, 0.2, 0.3, 0.4]))\n",
    "# random_dist = random_distribution()\n",
    "# print('random_dist = ', random_dist)\n",
    "# print('sample(random_dist) = ', sample(random_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "  * Training data is a list of matrices with input data. \n",
    "  * The size of the list equals to number of LSTM cells.\n",
    "  * The input is provided as 1-hot encoded letter (1-gram). The size of the 1-hot vector is vocabulary_size\n",
    "  * Each matrix has dimension batch_size x vocabulary_size\n",
    "  \n",
    "What is  num_nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294305 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      " priusuguf   legoszwzqzaz rhwrpd a y tplgihnhftndfri  x g ezcspch adrsm anaxtzdt\n",
      "cneevnz rqziwotg itzsc tqc  hptor c splph iroxeoeiuvcycign  xzr  dgp i  ioxiad w\n",
      "xqdutabsteid  izj y vlnewdaa  ixqfd htfdvpaxayh wjfs ienziwnt nnssdfimicvxb  lii\n",
      "sannreejh r iigleyvtnaoy ronrebrj y i a eao i e sgr wzezgedcvo rntrws  qndklno n\n",
      "fe elcjnzanr emjffvzzyea yk tie  jdnihr tsnkdnt pai  tpatisdujit  nzxao mpt apat\n",
      "================================================================================\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 100: 2.595111 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.44\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.260032 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.91\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 300: 2.100649 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 400: 1.995700 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 500: 1.937132 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 600: 1.907834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 700: 1.857770 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 800: 1.820286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.826439 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1000: 1.828093 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "================================================================================\n",
      "munced the aros shorman severueay a seer origurd cexsour or hhat authore in suef\n",
      "xer the arty is deweas it traman aechan s humnes countary devenated svetf of ten\n",
      "ong an one orefining of arges wen itting in ootent the elutces of ficclevench eb\n",
      "s inscous of the french the jcal esspreorative the avout hrorm he a genliet aul \n",
      "o  the fay eso dup the usembetering ioged that to six the gurs fallic recitly of\n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1100: 1.775972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1200: 1.750892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1300: 1.729582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.745648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1500: 1.738522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.742931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1700: 1.710100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1800: 1.671472 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1900: 1.639402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2000: 1.693200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "warle netal encomplits instingered bi pates has exiles to cremater towly gempia \n",
      "gate of the eight eight seven overne geener was renres in broth nons a reging ex\n",
      "chan sauphadied the hewhod progreltic a legey is one eight eight ome not and mor\n",
      "ems finem form one eight nine eight one nine eight five five four verono one nin\n",
      "battoma come alchrish place thereelys a lever and alsest in the not gequish prod\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100: 1.685588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2200: 1.676604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300: 1.644572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.659159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.673408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.647824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.658983 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2800: 1.644958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2900: 1.648189 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.645330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "h playe scowlend of b supersad in berition of seppared sy plot wayed s such stry\n",
      "blownsh is di misbers dendepwors office of plation hannattly bernal a rerded the\n",
      "vise includeses follious then as and learla americantion two sepredual tomal and\n",
      "berocity revially use sciet to rysparr two mindutbolade protim white collasine t\n",
      "ber theres infocuented and the athor olds rostroy thander waph is to gno spated \n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3100: 1.619792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3200: 1.644338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3300: 1.638335 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3400: 1.665750 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3500: 1.656879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3600: 1.673669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.643281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3800: 1.644197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.633981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.648471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "rees  infeeduan know and one six deleme deliki stoge pso the jare owing the arde\n",
      "gan vidgus and tole precodring one nine zero zero zero four five lomating and vi\n",
      "berorg the guinaqially the evense the enolt of arms one nine one have somings cu\n",
      "ourly ingregormal oction fulw the one five three state and mepual in coppeor dai\n",
      "way with a term of minsough one seven one seven three one cut hutbeodu passing t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4100: 1.629137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4200: 1.636612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4300: 1.611864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 4400: 1.608039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4500: 1.609377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4600: 1.611499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4700: 1.618224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4800: 1.628130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.628732 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5000: 1.605200 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "t or shated to this time to the resinter or movy he place roping one one nine ni\n",
      "garnt to advort to be eved one nine two brihta ajasted one eight nine five four \n",
      "geming refures enving treaking to him messech dutle motendupicifve bods into air\n",
      "cara reigtre sbatto till the econature to sently air lild jas clue symona fiffes\n",
      "catus as franth to not scott to runerusates stitary they instruch utson ofteg en\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5100: 1.602104 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.590639 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5300: 1.572769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5400: 1.570617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5500: 1.565115 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5600: 1.576511 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5700: 1.567203 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5800: 1.573518 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5900: 1.578309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6000: 1.546844 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "s as also ghote metal wn littlume rogod a religion moing with has president the \n",
      "manie can and maliman shown hand centais on the mawst councir musicilated ac it \n",
      "worous stotion unnument to de catiopeld not l tor controver fuel in typer s cont\n",
      "ound the atmandary the was arty and s anyphect one nine the strong ming of and t\n",
      "peish process with upeltical derived boy more of sleebe magus to boo is one nine\n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6100: 1.563337 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6200: 1.531706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6300: 1.539149 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6400: 1.536296 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6500: 1.553148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6600: 1.591395 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6700: 1.573985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6800: 1.602711 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6900: 1.578597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 7000: 1.575881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "jectons wakena mich for in one nine nine one its war is nameo vanuasia when the \n",
      "er one zero reckinatification miner this breasey of discultingly possibility thi\n",
      "phy are include in the was still to attentivation voses of western two five zero\n",
      "quide one nine two time and populatics it ta distert long stook definite organed\n",
      "y brey who air evidents by adise a morgoty dr bambraters but earey matable one e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.18\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
