{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term \n",
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print(text[:200])\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model.\n",
    "  * *num_unrollings* defines number of LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'abcdefghijklmnopqrstuwxyz'\n",
      "Batch size: 3, Segment: 8, unrolling: 2\n",
      "['abc', 'ijk', 'qrs']\n",
      "['cde', 'klm', 'stu']\n",
      "['efg', 'mno', 'uwx']\n",
      "['ghi', 'opq', 'xyz']\n",
      "['ijk', 'qrs', 'zab']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        # Segment is number of batches in the text\n",
    "        segment = self._text_size // batch_size\n",
    "        # Cursor is a list of pointers set to the beggining of each segment\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "    \n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "\n",
    "example = 'abcdefghijklmnopqrstuwxyz'\n",
    "a = BatchGenerator(example, 3, 2)\n",
    "print(\"Text: '%s'\" % example)\n",
    "print(\"Batch size: 3, Segment: %d, unrolling: 2\" % (len(example) // 3))\n",
    "for _ in range(5):\n",
    "    print(batches2string(a.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_distribution([0.1, 0.2, 0.3, 0.4]) = 3\n",
      "random_dist =  [[ 0.04888613  0.03143362  0.03878201  0.06217336  0.04456603  0.06394957\n",
      "   0.05993959  0.01149536  0.03189953  0.01030868  0.03388546  0.00362424\n",
      "   0.018982    0.04741691  0.05999234  0.03610939  0.04196859  0.00218992\n",
      "   0.05948233  0.00784825  0.06041797  0.02811769  0.06457286  0.02384245\n",
      "   0.03203228  0.06011389  0.01596954]]\n",
      "sample(random_dist) =  [[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "print('sample_distribution([0.1, 0.2, 0.3, 0.4]) = %d' % sample_distribution([0.1, 0.2, 0.3, 0.4]))\n",
    "random_dist = random_distribution()\n",
    "print('random_dist = ', random_dist)\n",
    "print('sample(random_dist) = ', sample(random_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "  * Training data is a list of matrices with input data. \n",
    "  * The size of the list equals to number of LSTM cells.\n",
    "  * The input is provided as 1-hot encoded letter (1-gram). The size of the 1-hot vector is vocabulary_size\n",
    "  * Each matrix has dimension batch_size x vocabulary_size\n",
    "  * num_nodes - Define number of units in each gate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # All gates: input, previous output, and bias.\n",
    "    X = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    M = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    B = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        gates = tf.matmul(i, X) + tf.matmul(o, M) + B\n",
    "        Yi, Yf, Yu, Yo = tf.split(1, 4, gates)\n",
    "        input_gate = tf.sigmoid(Yi)\n",
    "        forget_gate = tf.sigmoid(Yf)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(Yu)\n",
    "        output_gate = tf.sigmoid(Yo)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297192 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "n gle teoxz mficaclitqrrbnelemppjxni ciwvaogoletsirenvhai i  t t bkq oindtfwxrsn\n",
      "skqlnllcrsentrmtu xrhin  tclnobrelrqdt  jatzt et vlr n steup qcnaghz sc edpeltue\n",
      "vdxyt n eanqmas bcsbgawastoueszsftfifwkaheikb ih  zcxavzajvnj letrviscnfedfxsrfr\n",
      "qtaddmifijcfyatv gtncnsrjdjk  rnsmu dixklvlafjo  dkecjhbfumztzigrstxes x  eii uv\n",
      "z mcrnoccs  ia enkoi pxwvhli un wqvej abjb rp th aw  nh r uflniadcto hnefkhuvb a\n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.586340 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.14\n",
      "Validation set perplexity: 10.18\n",
      "Average loss at step 200: 2.244059 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 300: 2.100630 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 400: 2.007210 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 500: 1.948380 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 600: 1.914833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700: 1.863211 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 800: 1.827916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 900: 1.832207 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000: 1.826454 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "================================================================================\n",
      " which and myir empteting the sevens of the spect asp s int with of unit arilu g\n",
      "an a ligkay tur umen gener aukan this word will nackecur exarle it achuts of ber\n",
      "tive as the the six that shantitor for inharage fumun  uss he pars actrance acpa\n",
      "vers spribin south and puust tha s refetine the calf chenter vys gummente exprop\n",
      "s ling s mpolus wha the noply two niget seven the we whele i bone of wa rabry bu\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.776908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1200: 1.758567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1300: 1.735157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.747060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.745128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.745185 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1700: 1.717141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 1800: 1.673785 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1900: 1.651175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2000: 1.695380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      " one one eight three king eiecticion ins s hinduinnain which mackens labinition \n",
      "net themes ewritting ithuar watery world iically and forsing utiffen thared dild\n",
      "xmerse formaked from three egmes humbations algo s noter later freanlo referent \n",
      "phatt and gandy englenced lawish are and femail for levor hemperor the belwed st\n",
      "z be lagge cernated consuited oneent eavield tobly datter mueait are of the pors\n",
      "================================================================================\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2100: 1.687162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2200: 1.679527 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.641435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.665570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2500: 1.682053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600: 1.653043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2700: 1.661919 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2800: 1.648512 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 2900: 1.655530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3000: 1.651886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "ant newed in one nine six form apprognes is secord in american he s hiljwa which\n",
      "giabney and comporor satched with jubra copbity frised as mage and consulved to \n",
      "bassch person using amercost othere it speagts trikath phich play churchok expop\n",
      "wy it yokis to t innold kn en the cresty ulneovion one future to chower reside a\n",
      "wing the world one nine eight one nine fiveund to the tames codstouse players on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3100: 1.626846 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3200: 1.647490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3300: 1.635501 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.665709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3500: 1.660052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3600: 1.669963 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3700: 1.645649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.640244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3900: 1.638434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.653141 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "sla called begenditwaze althry buacknf the tough and one eight timent a emperor \n",
      "y with as only incladual cerloces for arguis war most emperolod of says who cons\n",
      "plants mays almos and mance et convidutive nibulliouses or one pensitional eight\n",
      "dey the one six ad to kinged when one eight times renurnon as four new hed and o\n",
      "ment sie vacters the tombitical malmit felpoentive sive of the the six eight one\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4100: 1.632933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.634071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.618870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4400: 1.612599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500: 1.613358 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4600: 1.621046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4700: 1.625574 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4800: 1.632039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4900: 1.632425 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5000: 1.609482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "els basthy be gond is declaso it is socra itslevrest be founder courrated five n\n",
      "ferile vasen and the humich one nine five the kennex a stan  and people as preyo\n",
      "jown in the sequent howeverly many the between like added conlinoted toway the u\n",
      "revoe ling rabols hay out the bysequent grogy of the researtic elopogy clovioney\n",
      "quetan mikterul jathmessel und vargeic be they orgent in scoveu recondieve ulra \n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5100: 1.601830 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.595898 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300: 1.579680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5400: 1.576713 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5500: 1.568860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5600: 1.580529 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5700: 1.573694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5800: 1.575007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5900: 1.574300 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6000: 1.546840 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "================================================================================\n",
      "y through hadder their the style protedren ng s shalinas by zehr it devate ethli\n",
      "urd among that cived at s to then accorduation froms andulan incample zebrales s\n",
      "gned a mernical of a monthed sapoksies thriek all emplost of the spouth juugned \n",
      "der sacced the sated by south age resouttly thind yeralle two zero zero zero fou\n",
      " malaw who two two zero mive three four zero feur it converted agoldrephere also\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6100: 1.567022 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.537269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6300: 1.544308 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6400: 1.542313 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6500: 1.554945 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6600: 1.594966 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.581062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.603491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.580623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7000: 1.576117 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "hyment presidental powilm states beliay subsict gooks teston king president day \n",
      "rived think one five mustist have leve aponsong r for the smn advolor prepict wa\n",
      "sio tans and reescord bra hadery comby prooutsut report ages webernangey the gam\n",
      "ne seven empode no as agaited as sociate the many playok a moracont pandale some\n",
      "jechum wivk the the lsom complete for the glone amictity on boge englton are a v\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
